{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory to Python path\n",
    "os.chdir(os.path.abspath('..'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ./data/eval\\miulab/tmlu. Loading from disk...\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader.tmlu_loader import TMLUDataLoader\n",
    "\n",
    "# TMLU dataset\n",
    "tmlu_loader = TMLUDataLoader(\"miulab/tmlu\")\n",
    "tmlu_dataset = tmlu_loader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation', 'subject', 'user_content'],\n",
       "        num_rows: 2796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation', 'subject', 'user_content'],\n",
       "        num_rows: 185\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlu_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset_as_messages(dataset_dict):\n",
    "    \"\"\"\n",
    "    Formats each row in the 'test' split of the dataset into a list of messages.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dict (DatasetDict): A DatasetDict containing 'test' and 'dev' splits.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of message lists for the 'test' split.\n",
    "    \"\"\"\n",
    "    # Define the fixed system role message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"\n",
    "    }\n",
    "\n",
    "    # Get the 'test' dataset\n",
    "    test_dataset = dataset_dict['test']\n",
    "    \n",
    "    # Initialize a list to hold the formatted messages\n",
    "    messages_list = []\n",
    "    \n",
    "    for example in test_dataset:\n",
    "        # Create the user role message using 'user_content' from the dataset\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example[\"user_content\"]\n",
    "        }\n",
    "        \n",
    "        # Combine the system message and user message into a list\n",
    "        messages = [system_message, user_message]\n",
    "        \n",
    "        # Add the list of messages to the final list\n",
    "        messages_list.append(messages)\n",
    "    \n",
    "    return messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = format_dataset_as_messages(tmlu_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful, pattern-following assistant that translates corporate jargon into plain English.'},\n",
       " {'role': 'user',\n",
       "  'content': '\\n        以下選擇題為出自臺灣的考題，答案為其中一個選項。\\n\\n        問題:\\n        閱讀下文，選出依序最適合填入□內的選項：\\n甲、我母親和我姑姑一同出洋去，上船的那天她伏在竹床上痛哭，綠衣綠裙上面釘有□□發光的小片子。(張愛玲〈私語〉)\\n乙、蝴蝶的本能是吮吸花蜜，女人的愛亦是一種本能：採集所有美好事物引誘自己進入想像，從自身記憶□□□□並且偷摘他人經驗之片段，想像繁殖成更豐饒的想像，織成一張華麗的密網。（簡媜〈母者〉）\\n丙、母親是天可汗，當家的天可汗，一家之王，絕對的威權，分配空間與食物的主人。她要我報告的事，或她突如其來要我□□的事，我最好都要知道，所以我在覲見可汗時，不論她問不問我話，我的心中就是會先有腹稿。(鍾文音〈我的天可汗〉)\\n\\n        (A) 張揚／綢繆未雨／奏疏 (B) 抽搐／煮繭抽絲／奏疏 (C) 張揚／煮繭抽絲／進貢\\t (D) 抽搐／綢繆未雨／進貢\\t\\n        正確答案：(\\n        '}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(example):\n",
    "    # Template for the question\n",
    "    prompt_template_part_one = '''\n",
    "    以下選擇題為出自臺灣的考題，答案為其中一個選項。\n",
    "\n",
    "    問題:\n",
    "    {question}\n",
    "\n",
    "    '''\n",
    "    prompt_template_part_two = '''\n",
    "    正確答案：(\n",
    "    '''\n",
    "    # List of possible options (A to F)\n",
    "    options = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    \n",
    "    # Check if each option exists in the example and construct answer string\n",
    "    answer_template = ' '.join([f\"({option}) {example[option]}\" for option in options if example.get(option)])\n",
    "    \n",
    "    # Fill the question in the prompt template\n",
    "    prompt = prompt_template_part_one.format(question=example[\"question\"]) + answer_template + prompt_template_part_two\n",
    "    \n",
    "    # Add the final prompt to 'user_content'\n",
    "    example['user_content'] = prompt\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 2796/2796 [00:05<00:00, 547.68 examples/s] \n",
      "Map (num_proc=2): 100%|██████████| 185/185 [00:04<00:00, 41.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = tmlu_dataset.map(create_user_prompt, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    以下選擇題為出自臺灣的考題，答案為其中一個選項。\\n\\n    問題:\\n    依商業會計法及商業會計處理準則之規定，下列敘述何者錯誤？\\n\\n    (A) 會計憑證分為原始溤證及記帳憑證 (B) 經濟部對於記帳憑證之名稱及格式並無規定 (C) 原始憑證係用以證明會計事項之經過, 而為造具記帳憑證所根據之憑證 (D) 記帳憑證可由代表商業負責人授權經理人、主辦或經辦會計人員簽名或蓋章\\n    正確答案：(\\n    '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['user_content'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation', 'subject'],\n",
       "        num_rows: 2796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation', 'subject'],\n",
       "        num_rows: 185\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlu_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'question', 'A', 'B', 'C', 'D', 'E', 'F', 'answer', 'explanation', 'metadata', 'human_evaluation'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names\n",
    "dataset_name = \"miulab/tmlu\"\n",
    "config=\"AST_chinese\"\n",
    "cache_dir = 'c:\\\\Users\\\\l501l\\\\Desktop\\\\TaiwanGPT\\\\data\\\\eval'\n",
    "load_dataset(dataset_name, config, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-4o-2024-08-06\",\n",
    "        \"gpt-4o-mini\",\n",
    "        \"gpt-4o-mini-2024-07-18\"\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-2024-08-06\n",
      "124 prompt tokens counted by num_tokens_from_messages().\n",
      "124 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o-mini\n",
      "124 prompt tokens counted by num_tokens_from_messages().\n",
      "124 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o-mini-2024-07-18\n",
      "124 prompt tokens counted by num_tokens_from_messages().\n",
      "124 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "_ = load_dotenv('.env')\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "        \"gpt-4o-2024-08-06\",\n",
    "        \"gpt-4o-mini\",\n",
    "        \"gpt-4o-mini-2024-07-18\"\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(model=model,\n",
    "    messages=example_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=1)\n",
    "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
